# -*- coding: utf-8 -*-
"""IA - Stacking - Aprendizado de Maquina.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HMQ34biOOjLyvCYcEmuauX0wMJzDaw9x

Bibliotecas básicas
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
import plotly.offline as py
import plotly.graph_objs as go
import random


CLOSED_ACCOUNT_CLASS_NAME = 'Attrited Customer'
DONT_CLOSED_ACCOUNT_CLASS_NAME = 'Existing Customer'
CLOSED_ACCOUNT_CLASS_VALUE = 1
DONT_CLOSED_ACCOUNT_CLASS_VALUE = 0

"""# Transformações dos dados

[Dataset Link
](https://www.kaggle.com/sakshigoyal7/credit-card-customers)

Leitura da base
"""

df=pd.read_csv('BankChurners.csv',sep=',')

df

"""Remove as colunas que aparentam ser resultados do Naive Bayes"""

df = df.drop([
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',
    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'
    ],axis=1)

"""Atualiza os valores da classe"""

df["Attrition_Flag"].replace({DONT_CLOSED_ACCOUNT_CLASS_NAME: DONT_CLOSED_ACCOUNT_CLASS_VALUE, 
                              CLOSED_ACCOUNT_CLASS_NAME: CLOSED_ACCOUNT_CLASS_VALUE}, inplace=True)

"""Distribuição de classes"""

sns.set(font_scale = 1.4)
sns_plot = sns.catplot(x="Attrition_Flag", kind="count", palette="ch:.25", data=df)
sns_plot.savefig("attrition.png")

sns.set(font_scale = 1.4)
sns_plot = sns.catplot(x="Gender", kind="count", palette="ch:.25", data=df)
sns_plot.savefig("gender.png")

sns.set(font_scale = 1.3)
sns_plot = sns.catplot(x="Education_Level", kind="count", palette="ch:.25", data=df, aspect=2)
sns_plot.savefig("education.png")

sns.set(font_scale = 1.4)
sns_plot = sns.catplot(x="Marital_Status", kind="count", palette="ch:.25", data=df, aspect=1.5)
sns_plot.savefig("marital.png")

sns.set(font_scale = 1.4)
sns_plot = sns.catplot(x="Income_Category", kind="count", palette="ch:.25", data=df, aspect=2)
sns_plot.savefig("income.png")

sns.set(font_scale = 1.4)
sns_plot = sns.catplot(x="Card_Category", kind="count", palette="ch:.25", data=df, aspect=1.0)
sns_plot.savefig("card.png")

#import matplotlib.pyplot as plt

#f, ax = plt.subplots(1, 1,figsize=(14,10))
#ax.plot(n_list,dist_list)
#ax.set_xlabel('n')
#ax.set_ylabel('dist')

class_count = [df.loc[df['Attrition_Flag'] == DONT_CLOSED_ACCOUNT_CLASS_VALUE].shape[0], df.loc[df['Attrition_Flag'] == CLOSED_ACCOUNT_CLASS_VALUE].shape[0]]

trace1 = go.Bar(x = [DONT_CLOSED_ACCOUNT_CLASS_NAME],
                y = [class_count[0]],
                text='%0.0f (%0.2f%%)' % (class_count[0], class_count[0]/df.shape[0] * 100),
                textposition='outside')
trace2 = go.Bar(x = [CLOSED_ACCOUNT_CLASS_NAME],
                y = [class_count[1]],
                text='%0.0f (%0.2f%%)' % (class_count[1], class_count[1]/df.shape[0] * 100),
                textposition='outside')
data = [trace1, trace2]

fig = go.Figure()
fig.update_layout(title='Attrition_Flag')
fig.add_trace(trace1)
fig.add_trace(trace2)
fig.show()

"""Primeiros elementos do dataset"""

df.head()

# 'CLIENTNUM',
# 'Customer_Age',
# 'Dependent_count',
# 'Months_on_book',
# 'Total_Relationship_Count',
# 'Months_Inactive_12_mon',
# 'Contacts_Count_12_mon',
# 'Credit_Limit',
# 'Total_Revolving_Bal',
# 'Avg_Open_To_Buy',
# 'Total_Amt_Chng_Q4_Q1',
# 'Total_Trans_Amt',
# 'Total_Trans_Ct',
# 'Total_Ct_Chng_Q4_Q1',
# 'Avg_Utilization_Ratio',

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Customer_Age", kde=True)
sns_plot.savefig("age.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Dependent_count", kde=True)
sns_plot.savefig("dependent.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Months_on_book", kde=True)
sns_plot.savefig("book.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Relationship_Count", kde=True)
sns_plot.savefig("relationship.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Months_Inactive_12_mon", kde=True)
sns_plot.savefig("inactive.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Contacts_Count_12_mon", kde=True)
sns_plot.savefig("contacts.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Credit_Limit", kde=True)
sns_plot.savefig("credit.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Revolving_Bal", kde=True)
sns_plot.savefig("revolution.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Avg_Open_To_Buy", kde=True)
sns_plot.savefig("open.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Amt_Chng_Q4_Q1", kde=True)
sns_plot.savefig("change.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Trans_Amt", kde=True)
sns_plot.savefig("amount.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Trans_Ct", kde=True)
sns_plot.savefig("transitions.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Total_Ct_Chng_Q4_Q1", kde=True)
sns_plot.savefig("count_change.png")

sns.set(font_scale = 1.3)
sns_plot = sns.displot(data=df, x="Avg_Utilization_Ratio", kde=True)
sns_plot.savefig("ratio.png")

"""Checando missing values"""

missing_list = []
for data in df.columns:
    if len(df[df[data].isna()]) > 0:
        missing_lis.append(data)
if len(missing_list) == 0:
    print('No missing values')
else:
    print(missing_list)

"""Transformação dos atributos categóricos"""

categorical_list = []
numerical_list = []
for column in df.columns:
    if df[column].describe().dtype == 'O':
        categorical_list.append(column)
    else:
        numerical_list.append(column)

categorical_dic = {}
for att in categorical_list:
    categorical_dic[att] = {}
    key = 0
    for data in df[att].unique():
        categorical_dic[att][data] = key
        key += 1

for att in categorical_list:
    df[att] = df[att].apply(lambda x: categorical_dic[att][x])

categorical_list

numerical_list

"""Separação X e y"""

y = df.iloc[:,1]
X = df.drop(['Attrition_Flag','CLIENTNUM'],axis=1)

X

"""Normalização min-max"""

X_norm = X.copy()
max_list = np.max(X)
min_list = np.min(X)
for i, max in enumerate(max_list):
    X_norm.iloc[:,i] = (X_norm.iloc[:,i] - min_list[i])/(max - min_list[i])

"""PCA """

from sklearn.decomposition import PCA
variance_list = []
components_list = []
for n in range(len(X.columns)+1):
    pca = PCA(n_components=n)
    pca.fit(X)
    variance_list.append(pca.explained_variance_ratio_.sum())
    components_list.append(n)

f, ax = plt.subplots(1, 1, figsize=(10,10))

ax.plot(components_list, variance_list)
#ax.set_xscale('log')
#ax.set_yscale('log')
ax.set_ylabel('variância', size = 15)
ax.set_xlabel('componentes', size = 15)
ax.tick_params(axis="x", labelsize=15)
ax.tick_params(axis="y", labelsize=15)
plt.savefig('pca.png')

"""Transformação da base"""

pca = PCA(n_components=5)
pca.fit(X)
X_pca = pca.transform(X)

"""PCA na base normalizada"""

variance_list = []
components_list = []
for n in range(len(X.columns)+1):
    pca = PCA(n_components=n)
    pca.fit(X_norm)
    variance_list.append(pca.explained_variance_ratio_.sum())
    components_list.append(n)

f, ax = plt.subplots(1, 1, figsize=(10,10))

ax.plot(components_list, variance_list)
#ax.set_xscale('log')
#ax.set_yscale('log')
ax.set_ylabel('variância', size = 15)
ax.set_xlabel('componentes', size = 15)
ax.tick_params(axis="x", labelsize=15)
ax.tick_params(axis="y", labelsize=15)
plt.savefig('pca_norm.png')

"""# Classificação"""

from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import RBF
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_val_score, cross_val_predict, StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn import metrics
import time

K_VALUE = 10
seed = 7
stratifiedKfold = StratifiedKFold(n_splits=K_VALUE, random_state=seed, shuffle=True)

"""## Funções (Acurácia, Precision, Recall, F1, Matriz de confusao, Curva ROC)"""

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_fscore_support

def metricas_de_avaliacao(y_true, y_pred, label):
  return pd.Series({'accuracy': accuracy_score(y_true, y_pred),
                    'precision': precision_score(y_true, y_pred, zero_division=0),
                    'recall': recall_score(y_true, y_pred),
                    'f1': f1_score(y_true, y_pred)},
                   name=label)
  
def gerar_metricas_de_avaliacao(clf_names, param_descs, dataset_names, clf_list, X_list, y, cv, file_name):
  pd_series_list = []
  for i in range(0, len(clf_list)):
    y_pred = cross_val_predict(clf_list[i], X_list[i], y, cv=cv)
    #pd_series_list.append(metricas_de_avaliacao(y, y_pred, '%s  %s  %s' % (clf_names[i], param_descs[i], dataset_names[i])))
    pd_series_list.append(metricas_de_avaliacao(y, y_pred, '%s' % (clf_names[i])))
    
  pd_metrics = pd.concat(pd_series_list, axis=1)
  pd_metrics.to_csv("%s.csv" % (file_name))
  return pd_metrics

from sklearn.metrics import confusion_matrix

def create_conf_matrix_pd(conf_matrix):
  conf_matrix = np.array(conf_matrix)
  return pd.concat([conf_matrix_pd(conf_matrix[:,0], 'P'),
                    conf_matrix_pd(conf_matrix[:,1], 'N')],
                    axis=1)

def conf_matrix_pd(conf_matrix_class_result, label):
  return pd.Series({'P': conf_matrix_class_result[0],
                    'N': conf_matrix_class_result[1]},
                    name=label)

def gerar_matriz_de_confusao(clf_names, param_descs, dataset_names, clf_list, X_list, y, cv):
  plt.rcParams.update(plt.rcParamsDefault)
  for i in range(0, len(clf_list)):
    y_pred = cross_val_predict(clf_list[i], X_list[i], y, cv=cv)
    conf_matrix = confusion_matrix(y, y_pred)
    pd_conf_matrix = create_conf_matrix_pd(conf_matrix)
    #pd_conf_matrix.to_csv("%s_%s_matriz_de_confusao_%s.csv" % (clf_names[i], param_descs[i], dataset_names[i]))
    pd_conf_matrix.to_csv("%s_matriz_de_confusao.csv" % (clf_names[i]))
    group_names = ['TP','FP','FN','TN']
    group_counts = ["{0:0.0f}".format(value) for value in np.array(conf_matrix).flatten()]
    group_percentages = ["{0:.2%}".format(value) for value in np.array(conf_matrix).flatten()/np.sum(np.array(conf_matrix))]
    labels = [f"{v1}\n{v2}\n{v3}" for v1, v2, v3 in zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)
    f, ax = plt.subplots(figsize=(3, 2))
    conf_matrix_plot = sns.heatmap(pd_conf_matrix, annot=labels, fmt='', cmap="RdBu", annot_kws={"fontsize":10}, ax=ax)
    #conf_matrix_plot.set_title('%s - %s - %s'  % (clf_names[i], param_descs[i], dataset_names[i]))
    conf_matrix_plot.set_title('%s'  % (clf_names[i]))
    conf_matrix_plot.set_xticklabels(conf_matrix_plot.get_xmajorticklabels(), fontsize = 12)
    conf_matrix_plot.set_yticklabels(conf_matrix_plot.get_ymajorticklabels(), fontsize = 12)


    #conf_matrix_plot.figure.savefig("%s_%s_matriz_de_confusao_%s.png" % (clf_names[i], param_descs[i], dataset_names[i]))
    conf_matrix_plot.figure.savefig("%s_matriz_de_confusao.png" % (clf_names[i]))
    plt.show()
    plt.clf()

def gerar_curva_roc(clf_names, param_descs, dataset_names, clf_list, X_list, y, cv):
  plt.rcParams.update(plt.rcParamsDefault)
  for i in range(0, len(clf_list)):
    probs = cross_val_predict(clf_list[i], X_list[i], y, cv=cv, method='predict_proba')
    fpr, tpr, threshold = metrics.roc_curve(y, probs[:,1])
    roc_auc = metrics.auc(fpr, tpr)

    plt.figure()
    #plt.title('%s - %s - %s' % (clf_names[i], param_descs[i], dataset_names[i]))
    plt.title('%s' % (clf_names[i]))
    plt.plot(fpr, tpr, 'b', label = '%s (AUC = %.3f)' % (clf_names[i],roc_auc))
    plt.legend(loc = 'lower right')
    #plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([-0.05, 1.05])
    plt.ylim([-0.05, 1.05])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    #plt.savefig("%s_%s_curva_roc_%s.png" % (clf_names[i], param_descs[i], dataset_names[i]))
    plt.savefig("%s_curva_roc.png" % (clf_names[i]))

"""## Árvore - Estimativa de parâmetros"""

from sklearn.tree import DecisionTreeClassifier
import time
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.model_selection import train_test_split
from sklearn import preprocessing

dataset_names = ['default', 'normalized', 'pca']
dataset_list = [X, X_norm, X_pca]

clf = DecisionTreeClassifier()
clf_list = []
depth_list = []

for i in range(1,101):
  clf = DecisionTreeClassifier(max_depth=i)
  clf_list.append(clf)
  depth_list.append(i)

#Irá armazenar os melhores de cada base
decision_tree_means_scores = []
decision_tree_best_scores = []
decision_tree_best_scores_std = []
decision_tree_best_depths = []
decision_tree_mean_scores = []
decision_tree_mean_scores_std = []

for i in range(0, len(dataset_names)):
  text_file = open('decision_tree_estimativa_parametros_%s.txt' % (dataset_names[i]), 'w')
  text_file.write("Arvore de Decisao \n")
  text_file.write("Base %s \n" % (dataset_names[i]))
  text_file.write("======================\n")
  text_file.write("Individual\n")

  scores_list = []
  time_list = []
  for clf in clf_list:
    inicio = time.time()
    scores = cross_val_score(clf, dataset_list[i], y, cv=stratifiedKfold)
    scores_list.append(scores)
    fim = time.time()
    time_list.append(fim - inicio)
    
    text_file.write("\nmax_depth = %0.0f - Accuracy: %0.4f (+/- %0.4f) \n" % (clf.max_depth, np.mean(scores), np.std(scores)))
    kfoldAcurracyLine = "\t k-fold (k = %0.0f) Accuracy = [" % (K_VALUE)
    for j in range(0, len(scores)):
      if j == len(scores) -1:
        kfoldAcurracyLine += " %0.4f ]" % (scores[j])
      else:
        kfoldAcurracyLine += " %0.4f ," % (scores[j])
    text_file.write(kfoldAcurracyLine + "\n")


  scores_list = np.array(scores_list)
  mean_scores = scores_list.mean(axis=1)

  best_score = np.max(mean_scores)
  index_best = np.argmax(mean_scores)
  best_depth = depth_list[index_best]
  best_score_std = np.std(scores_list[index_best])

  text_file.write("======================\n")
  text_file.write("\n")
  text_file.write("======================\n")
  text_file.write("Melhor resultado: \n")
  text_file.write("max_depth = %0.0f - Accuracy: %0.4f (+/- %0.4f) \n" % (best_depth, best_score, best_score_std))
  text_file.write("\n")
  text_file.write("Média das Acurácias: %0.4f\n" % (mean_scores.mean()))
  text_file.write("======================\n")
  text_file.close()
  
  decision_tree_means_scores.append(mean_scores)
  decision_tree_best_scores.append(best_score)
  decision_tree_best_scores_std.append(best_score_std)
  decision_tree_best_depths.append(best_depth)

  print("===== base %s =====" % dataset_names[i])
  print("best_score %0.4f" % best_score)
  print("best_score_std %0.4f" % best_score_std)
  print("max_depth %0.0f" % best_depth)
  print("mean_score %0.4f" % mean_scores.mean())
  print("mean_score_std %0.4f" % mean_scores.std())
  print()

"""Resultados em cada base"""

def decision_tree_infos(max_depth, best_score, best_score_std, mean_score, mean_score_std, label):
  return pd.Series({'max_depth': '%0.0f' % max_depth,
                    'Best Score': '%0.4f' %  best_score,
                    'Best Score Std': '%0.4f' %  best_score_std,
                    'Mean Score': '%0.4f' % mean_score,
                    'Mean Score Std': '%0.4f' % mean_score_std},
                    name=label)


pd_series_list = []

for i in range(0, len(dataset_names)):
  	pd_series_list.append(decision_tree_infos(
                            decision_tree_best_depths[i],
                            decision_tree_best_scores[i],
                            decision_tree_best_scores_std[i],
                            decision_tree_means_scores[i].mean(),
                            decision_tree_means_scores[i].std(), 
                            dataset_names[i]))

pd.concat(pd_series_list, axis=1)

decision_tree_pd = pd.concat(pd_series_list, axis=1)
decision_tree_pd.to_csv("decision_tree_resultado_melhores_parametros.csv")
decision_tree_pd

"""Acurácia, Precision, Recall, F1"""

clf_names = [
   "AD max_depth = 7 default",
   "AD max_depth = 7 normalized",
   "AD max_depth = 7 pca"
]

dataset_names = [
    'default', 
    'normalized', 
    'pca'
]

clf_list = [
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=7)
]

param_descs = [
   "k=%0.0f" % (clf_list[0].max_depth),
   "k=%0.0f" % (clf_list[1].max_depth),
   "k=%0.0f" % (clf_list[2].max_depth)
]

X_list = [
   X,
   X_norm,
   X_pca
]

gerar_metricas_de_avaliacao(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold, 'decision_tree_resultado_melhores_parametros_metricas_de_avaliacao')

"""Gráfico de acurácia e tempo"""

dataset_names = ['Padrão', 'Normalizado', 'PCA']
for i in range(0, len(dataset_names)):
  #f, ax = plt.subplots(1, 2, figsize=(14,10))
  f, ax = plt.subplots()
  ax.set_title('Árvore de decisão - %s' % (dataset_names[i]))
  ax.set_xlabel('max_depth',size=15)
  ax.set_ylabel('Acurácia',size=15)
  ax.tick_params(axis="x", labelsize=15)
  ax.tick_params(axis="y", labelsize=15)
  ax.plot(depth_list,decision_tree_means_scores[i])
  #ax[1].set_title('Decision Tree - %s' % (dataset_names[i]))
  #ax[1].set_xlabel('max_depth',size=12)
  #ax[1].set_ylabel('Time (seconds)',size=12)
  #ax[1].plot(depth_list,decision_tree_train_times[i])

  plt.savefig('decision_tree_acuracia_%s.png' % dataset_names[i], bbox_inches = "tight")
  plt.show()

"""## Naive Bayes - Estimativa de parâmetros"""

import time
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import CategoricalNB


gaussian_time = time.time()
clf = GaussianNB()
gaussian_score = cross_val_score(clf, X, y, cv=stratifiedKfold).mean()
gaussian_time = time.time() - gaussian_time

bernoulli_time = time.time()
clf = BernoulliNB()
bernoulli_score = cross_val_score(clf, X, y, cv=stratifiedKfold).mean()
bernoulli_time = time.time() - bernoulli_time

complement_time = time.time()
clf = ComplementNB()
complement_score = cross_val_score(clf, X, y, cv=stratifiedKfold).mean()
complement_time = time.time() - complement_time

multinomial_time = time.time()
clf = MultinomialNB()
multinomial_score = cross_val_score(clf, X, y, cv=stratifiedKfold).mean()
multinomial_time = time.time() - multinomial_time

nb_pd = pd.concat([pd.Series({'Gaussian': gaussian_score,
           'BernoulliNB': bernoulli_score,
           'Complement': complement_score,
           'Multinomial': multinomial_score},
            name='accuracy'), 
           pd.Series({'Gaussian': gaussian_time,
           'BernoulliNB': bernoulli_time,
           'Complement': complement_time,
           'Multinomial': multinomial_time},
            name='time')
           ],axis = 1)

nb_pd = nb_pd.sort_values(by=['accuracy'], ascending=False)
nb_pd.to_csv("diferentes_naive_bayes_comparacao_default.csv")
nb_pd

import time
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn import preprocessing
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import ComplementNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import CategoricalNB

dataset_names = ['default', 'normalized', 'pca']
dataset_list = [X, X_norm, X_pca]

clf = GaussianNB()
clf_list = []
smoothing_list = []

#smoothing = 0.0000000001
smoothing = 0.00000000000000001
for i in range(0,400):
  if i > 0:
    #smoothing = smoothing + 0.0000000125
    smoothing = smoothing + 0.00000000000000000125
  clf = GaussianNB(var_smoothing=smoothing)
  smoothing_list.append(smoothing)
  clf_list.append(clf)


#Irá armazenar os melhores de cada base
gaussian_nb_means_scores = []
gaussian_nb_best_scores = []
gaussian_nb_best_scores_std = []
gaussian_nb_best_var_smoothing = []
gaussian_nb_mean_scores = []
gaussian_nb_mean_scores_std = []

for i in range(0, len(dataset_names)):
  text_file = open('gaussian_nb_estimativa_parametros_%s.txt' % (dataset_names[i]), 'w')
  text_file.write("Gaussian Naive Bayes \n")
  text_file.write("Base %s \n" % (dataset_names[i]))
  text_file.write("======================\n")
  text_file.write("Individual\n")

  scores_list = []
  time_list = []
  for clf in clf_list:
    inicio = time.time()
    scores = cross_val_score(clf, dataset_list[i], y, cv=stratifiedKfold)
    scores_list.append(scores)
    fim = time.time()
    time_list.append(fim - inicio)
    
    text_file.write("\nvar_smoothing = %0.20f - Accuracy: %0.4f (+/- %0.4f) \n" % (clf.var_smoothing, np.mean(scores), np.std(scores)))
    kfoldAcurracyLine = "\t k-fold (k = %0.0f) Accuracy = [" % (K_VALUE)
    for j in range(0, len(scores)):
      if j == len(scores) -1:
        kfoldAcurracyLine += " %0.4f ]" % (scores[j])
      else:
        kfoldAcurracyLine += " %0.4f ," % (scores[j])
    text_file.write(kfoldAcurracyLine + "\n")


  scores_list = np.array(scores_list)
  mean_scores = scores_list.mean(axis=1)

  best_score = np.max(mean_scores)
  index_best = np.argmax(mean_scores)
  best_var_smoothing = smoothing_list[index_best]
  best_score_std = np.std(scores_list[index_best])

  text_file.write("======================\n")
  text_file.write("\n")
  text_file.write("======================\n")
  text_file.write("Melhor resultado: \n")
  text_file.write("var_smoothing = %0.20f - Accuracy: %0.4f (+/- %0.4f) \n" % (best_var_smoothing, best_score, best_score_std))
  text_file.write("\n")
  text_file.write("Média das Acurácias: %0.4f\n" % (mean_scores.mean()))
  text_file.write("======================\n")
  text_file.close()
  
  gaussian_nb_means_scores.append(mean_scores)
  gaussian_nb_best_scores.append(best_score)
  gaussian_nb_best_scores_std.append(best_score_std)
  gaussian_nb_best_var_smoothing.append(best_var_smoothing)

  print("===== base %s =====" % dataset_names[i])
  print("best_score %0.4f" % best_score)
  print("best_score_std %0.4f" % best_score_std)
  print("best_var_smoothing %0.20f" % best_var_smoothing)
  print("mean_score %0.4f" % mean_scores.mean())
  print("mean_score_std %0.4f" % mean_scores.std())
  print()

def gaussian_naive_bayes_infos(var_smoothing, best_score, best_score_std, mean_score, mean_score_std, label):
  return pd.Series({'var_smoothing': '%0.20f' % var_smoothing,
                    'Best Score': '%0.4f' %  best_score,
                    'Best Score Std': '%0.4f' %  best_score_std,
                    'Mean Score': '%0.4f' % mean_score,
                    'Mean Score Std': '%0.4f' % mean_score_std},
                    name=label)


pd_series_list = []

for i in range(0, len(dataset_names)):
  	pd_series_list.append(gaussian_naive_bayes_infos(
                            gaussian_nb_best_var_smoothing[i],
                            gaussian_nb_best_scores[i],
                            gaussian_nb_best_scores_std[i],
                            gaussian_nb_means_scores[i].mean(),
                            gaussian_nb_means_scores[i].std(), 
                            dataset_names[i]))

gaussian_nb_pd = pd.concat(pd_series_list, axis=1)
gaussian_nb_pd.to_csv("gaussian_nb_resultado_melhores_parametros.csv")
gaussian_nb_pd

"""Acurácia, Precision, Recall, F1"""

clf_names = [
   "AD var_smoothing = 1e-9 default",
   "AD var_smoothing = 1e-9 normalized",
   "AD var_smoothing = 1e-9 pca"
]

dataset_names = [
    'default', 
    'normalized', 
    'pca'
]

clf_list = [
    GaussianNB(),
    GaussianNB(),
    GaussianNB()
]

param_descs = [
   "var_smoothing=%0.10f" % (clf_list[0].var_smoothing),
   "var_smoothing=%0.10f" % (clf_list[1].var_smoothing),
   "var_smoothing=%0.10f" % (clf_list[2].var_smoothing)
]

X_list = [
   X,
   X_norm,
   X_pca
]

gerar_metricas_de_avaliacao(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold, 'gaussian_nb_resultado_melhores_parametros_metricas_de_avaliacao')

"""Gráfico de acurácia e tempo"""

dataset_names = ['Padrão', 'Normalizado', 'PCA']
for i in range(0, len(dataset_names)):
  #f, ax = plt.subplots(1, 2, figsize=(14,10))
  f, ax = plt.subplots()
  ax.set_title('Gaussian Naive Bayes - %s' % (dataset_names[i]))
  ax.set_xlabel('var_smoothing',size=15)
  ax.set_ylabel('Acurácia',size=15)
  ax.tick_params(axis="x", labelsize=15)
  ax.tick_params(axis="y", labelsize=15)
  ax.plot(smoothing_list,gaussian_nb_means_scores[i])
  #ax[1].set_title('Gaussian Naive Bayes - %s' % (dataset_names[i]))
  #ax[1].set_xlabel('var_smoothing',size=12)
  #ax[1].set_ylabel('Time (seconds)',size=12)
  #ax[1].plot(smoothing_list,gaussian_naive_bayes_train_times[i])

  plt.savefig('gaussian_nb_acuracia_%s.png' % dataset_names[i], bbox_inches = "tight")
  plt.show()

"""## kNN - Estimativa de parâmetros"""

from sklearn.neighbors import KNeighborsClassifier
import time
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_score, StratifiedKFold

dataset_names = ['default', 'normalized', 'pca']
dataset_list = [X, X_norm, X_pca]

clf_list = []
k_list = []

for i in range(1,30):
  clf = clf = KNeighborsClassifier(n_neighbors=i)
  clf_list.append(clf)
  k_list.append(i)


#Irá armazenar os melhores de cada base
kNN_means_scores = []
kNN_best_scores = []
kNN_best_scores_std = []
kNN_best_k = []
kNN_mean_scores = []
kNN_mean_scores_std = []

for i in range(0, len(dataset_names)):
  text_file = open('kNN_estimativa_parametros_%s.txt' % (dataset_names[i]), 'w')
  text_file.write("kNN \n")
  text_file.write("Base %s \n" % (dataset_names[i]))
  text_file.write("======================\n")
  text_file.write("Individual\n")

  scores_list = []
  time_list = []
  for clf in clf_list:
    inicio = time.time()
    scores = cross_val_score(clf, dataset_list[i], y, cv=stratifiedKfold)
    scores_list.append(scores)
    fim = time.time()
    time_list.append(fim - inicio)
    
    text_file.write("\nk = %0.0f - Accuracy: %0.4f (+/- %0.4f) \n" % (clf.n_neighbors, np.mean(scores), np.std(scores)))
    kfoldAcurracyLine = "\t k-fold (k = %0.0f) Accuracy = [" % (K_VALUE)
    for j in range(0, len(scores)):
      if j == len(scores) -1:
        kfoldAcurracyLine += " %0.4f ]" % (scores[j])
      else:
        kfoldAcurracyLine += " %0.4f ," % (scores[j])
    text_file.write(kfoldAcurracyLine + "\n")


  scores_list = np.array(scores_list)
  mean_scores = scores_list.mean(axis=1)

  best_score = np.max(mean_scores)
  index_best = np.argmax(mean_scores)
  best_k = k_list[index_best]
  best_score_std = np.std(scores_list[index_best])

  text_file.write("======================\n")
  text_file.write("\n")
  text_file.write("======================\n")
  text_file.write("Melhor resultado: \n")
  text_file.write("k = %0.0f - Accuracy: %0.4f (+/- %0.4f) \n" % (best_k, best_score, best_score_std))
  text_file.write("\n")
  text_file.write("Média das Acurácias: %0.4f\n" % (mean_scores.mean()))
  text_file.write("======================\n")
  text_file.close()
  
  kNN_means_scores.append(mean_scores)
  kNN_best_scores.append(best_score)
  kNN_best_scores_std.append(best_score_std)
  kNN_best_k.append(best_k)

  print("===== base %s =====" % dataset_names[i])
  print("best_score %0.4f" % best_score)
  print("best_score_std %0.4f" % best_score_std)
  print("best_k %0.0f" % best_k)
  print("mean_score %0.4f" % mean_scores.mean())
  print("mean_score_std %0.4f" % mean_scores.std())
  print()

def kNN_infos(k, best_score, best_score_std, mean_score, mean_score_std, label):
  return pd.Series({'k': '%0.0f' % k,
                    'Best Score': '%0.4f' %  best_score,
                    'Best Score Std': '%0.4f' %  best_score_std,
                    'Mean Score': '%0.4f' % mean_score,
                    'Mean Score Std': '%0.4f' % mean_score_std},
                    name=label)


pd_series_list = []

for i in range(0, len(dataset_names)):
  	pd_series_list.append(kNN_infos(
                            kNN_best_k[i],
                            kNN_best_scores[i],
                            kNN_best_scores_std[i],
                            kNN_means_scores[i].mean(),
                            kNN_means_scores[i].std(), 
                            dataset_names[i]))

kNN_pd = pd.concat(pd_series_list, axis=1)
kNN_pd.to_csv("kNN_resultado_melhores_parametros.csv")
kNN_pd

"""Acurácia, Precision, Recall, F1"""

clf_names = [
   "kNN k = 9 default",
   "kNN k = 11 normalized",
   "kNN k = 9 pca"
]

dataset_names = [
    'default', 
    'normalized', 
    'pca'
]

clf_list = [
    KNeighborsClassifier(n_neighbors=9),
    KNeighborsClassifier(n_neighbors=11),
    KNeighborsClassifier(n_neighbors=9)
]

param_descs = [
   "k=%0.0f" % (clf_list[0].n_neighbors),
   "k=%0.0f" % (clf_list[1].n_neighbors),
   "k=%0.0f" % (clf_list[2].n_neighbors)
]



X_list = [
   X,
   X_norm,
   X_pca
]

gerar_metricas_de_avaliacao(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold, 'kNN_resultado_melhores_parametros_metricas_de_avaliacao')

"""Gráfico de acurácia e tempo"""

dataset_names = ['Padrão', 'Normalizado', 'PCA']
for i in range(0, len(dataset_names)):
  #f, ax = plt.subplots(1, 2, figsize=(14,10))
  f, ax = plt.subplots()
  ax.set_title('kNN - %s' % (dataset_names[i]))
  ax.set_xlabel('k',size=15)
  ax.set_ylabel('Acurácia',size=15)
  ax.plot(k_list,kNN_means_scores[i])
  ax.tick_params(axis="x", labelsize=15)
  ax.tick_params(axis="y", labelsize=15)
  #ax[1].set_title('Gaussian Naive Bayes - %s' % (dataset_names[i]))
  #ax[1].set_xlabel('var_smoothing',size=12)
  #ax[1].set_ylabel('Time (seconds)',size=12)
  #ax[1].plot(smoothing_list,gaussian_naive_bayes_train_times[i])

  plt.savefig('kNN_acuracia_%s.png' % dataset_names[i], bbox_inches = "tight")

"""## Resultado Individuais dos classificadores

Acurácia, precision, recall, f1
"""

clf_names = [
   "kNN1",
   "kNN2",
   "kNN3",
   "kNN4",
   "NB1",
   "NB2",
   "AD1",
   "AD2",
   "AD3",
   "AD4"
]

dataset_names = [
    'normalized', 
    'pca', 
    'pca',
    'pca', 
    'default', 
    'pca',
    'default', 
    'normalized', 
    'normalized',
    'normalized'
]

clf_list = [
    KNeighborsClassifier(n_neighbors=11),
    KNeighborsClassifier(n_neighbors=1),
    KNeighborsClassifier(n_neighbors=9),
    KNeighborsClassifier(n_neighbors=20),
    GaussianNB(),
    GaussianNB(),
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=2),
    DecisionTreeClassifier(max_depth=20)
]

param_descs = [
   "k=%0.0f" % (clf_list[0].n_neighbors),
   "k=%0.0f" % (clf_list[1].n_neighbors),
   "k=%0.0f" % (clf_list[2].n_neighbors),
   "k=%0.0f" % (clf_list[3].n_neighbors),
   "var_smoothing=%0.10f" % (clf_list[4].var_smoothing),
   "var_smoothing=%0.10f" % (clf_list[5].var_smoothing),
   "max_depth=%0.0f" % (clf_list[6].max_depth),
   "max_depth=%0.0f" % (clf_list[7].max_depth),
   "max_depth=%0.0f" % (clf_list[8].max_depth),
   "max_depth=%0.0f" % (clf_list[9].max_depth)
]



X_list = [
   X_norm,
   X_pca,
   X_pca,
   X_pca,
   X,
   X_pca,
   X,
   X_norm,
   X_norm,
   X_norm
]

gerar_metricas_de_avaliacao(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold, 'classificadores_para_stacking_metricas_de_avaliacao')

gerar_matriz_de_confusao(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold)

gerar_curva_roc(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold)

"""## Stacking"""

from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import StackingClassifier
from sklearn.pipeline import make_pipeline

min_max = MinMaxScaler()
pca = PCA(n_components=5)

clf1 = KNeighborsClassifier(n_neighbors=11) # será aplicado na base normalizada
clf2 = KNeighborsClassifier(n_neighbors=1) # será aplicado na base com pca
clf3 = KNeighborsClassifier(n_neighbors=9) # será aplicado na base com pca
clf4 = KNeighborsClassifier(n_neighbors=20) # será aplicado na base com pca
clf5 = GaussianNB()
clf6 = GaussianNB() # será aplicado na base com pca
clf7 = DecisionTreeClassifier(max_depth=7)
clf8 = DecisionTreeClassifier(max_depth=7) # será aplicado na base normalizada
clf9 = DecisionTreeClassifier(max_depth=2) # será aplicado na base normalizada
clf10 = DecisionTreeClassifier(max_depth=20) # será aplicado na base normalizada

pipelines = []

pipe1 = make_pipeline(min_max, clf1)
pipelines.append(['kNN k=11 (Normalizada)',pipe1])

pipe2 = make_pipeline(pca, clf2)
pipelines.append(['kNN k=1 (PCA)',pipe2])

pipe3 = make_pipeline(pca, clf3)
pipelines.append(['kNN k=9 (PCA)',pipe3])

pipe4 = make_pipeline(pca, clf4)
pipelines.append(['kNN k=20 (PCA)',pipe4])

pipe5 = make_pipeline(clf5)
pipelines.append(['GausisianNB',pipe5])

pipe6 = make_pipeline(pca, clf6)
pipelines.append(['GausisianNB (PCA)',pipe6])

pipe7 = make_pipeline(clf7)
pipelines.append(['Decision Tree max_depth=7',pipe7])

pipe8 = make_pipeline(min_max, clf8)
pipelines.append(['Decision Tree max_depth=7 (Normalizada)',pipe8])

pipe9 = make_pipeline(min_max, clf9)
pipelines.append(['Decision Tree max_depth=2 (Normalizada)',pipe9])

pipe10 = make_pipeline(min_max, clf10)
pipelines.append(['Decision Tree max_depth=20 (Normalizada)',pipe10])

print(len(pipelines))

print(pipe1)
print(pipe2)
print(pipe3)
print(pipe4)
print(pipe5)
print(pipe6)
print(pipe7)
print(pipe8)
print(pipe9)
print(pipe10)

"""Stacking com kNN no Final"""

kNN_sclf = StackingClassifier(estimators=pipelines, final_estimator=KNeighborsClassifier(), cv=stratifiedKfold)
kNN_sclf_scores = cross_val_score(kNN_sclf, X, y, cv=stratifiedKfold)
kNN_sclf_scores = np.array(kNN_sclf_scores)

print('accuracy: %0.4f (+/- %0.4f)' % (kNN_sclf_scores.mean(), kNN_sclf_scores.std()))

"""Stacking com Gaussian Naive Bayes no Final"""

gaussian_nb_sclf = StackingClassifier(estimators=pipelines, final_estimator=GaussianNB(), cv=stratifiedKfold)
gaussian_nb_sclf_scores = cross_val_score(gaussian_nb_sclf, X, y, cv=stratifiedKfold)
gaussian_nb_sclf_scores = np.array(gaussian_nb_sclf_scores)

print('accuracy: %0.4f (+/- %0.4f)' % (gaussian_nb_sclf_scores.mean(), gaussian_nb_sclf_scores.std()))

"""Stacking com Arvore de Decisao no Final"""

decision_tree_sclf = StackingClassifier(estimators=pipelines, final_estimator=DecisionTreeClassifier(), cv=stratifiedKfold)
decision_tree_sclf_scores = cross_val_score(decision_tree_sclf, X, y, cv=stratifiedKfold)
decision_tree_sclf_scores = np.array(decision_tree_sclf_scores)

print('accuracy: %0.4f (+/- %0.4f)' % (decision_tree_sclf_scores.mean(), decision_tree_sclf_scores.std()))

"""Comparando resultados dos Stacking"""

stacking_pd = pd.concat([pd.Series({'kNN': kNN_sclf_scores.mean(),
                          'Gaussian NB': gaussian_nb_sclf_scores.mean(),
                          'Decision Tree': decision_tree_sclf_scores.mean()},
                            name='accuracy'),
                         pd.Series({'kNN': kNN_sclf_scores.std(),
                          'Gaussian NB': gaussian_nb_sclf_scores.std(),
                          'Decision Tree': decision_tree_sclf_scores.std()},
                            name='std')
                          ],axis = 1)

stacking_pd = stacking_pd.sort_values(by=['accuracy'], ascending=False)
stacking_pd.to_csv("stacking_comparacao_de_desempenho.csv")
stacking_pd

sclf = kNN_sclf

"""Acurácia, precision, recall e f1"""

gerar_metricas_de_avaliacao(['Stacking'], [''], ['default'], [sclf], [X], y, stratifiedKfold, 'stacking_metricas_de_avaliacao')

"""Matriz de confusão"""

gerar_matriz_de_confusao(['Stacking'], [''], ['default'], [sclf], [X], y, stratifiedKfold)

"""Curva ROC"""

gerar_curva_roc(['Stacking'], [''], ['default'], [sclf], [X], y, stratifiedKfold)

"""### Comparaçao de Curvas ROC Stacking e classificadores individuais"""

def gerar_comparacao_curva_roc(clf_names, param_descs, dataset_names, clf_list, X_list, y, cv):
  plt.rcParams.update(plt.rcParamsDefault)
  color = ['b','g','r','c','m','b','g','r','c','m','k']
  style = ['-.','--',':','-','--',':','-.','--',':','-','-']
  for i in range(0, len(clf_list)):
      probs = cross_val_predict(clf_list[i], X_list[i], y, cv=cv, method='predict_proba')
      fpr, tpr, threshold = metrics.roc_curve(y, probs[:,1])
      roc_auc = metrics.auc(fpr, tpr)

      plt.plot(fpr, tpr, 'b', label = '%s (AUC = %.3f)' % (clf_names[i],roc_auc), color=color[i], linestyle=style[i])
  #plt.figure()
  #plt.title('%s - %s - %s' % (clf_names[i], param_descs[i], dataset_names[i]))
  plt.legend(loc = 'lower right',fontsize=11)
  #plt.plot([0, 1], [0, 1],'r--')
  plt.xlim([-0.05, 1.05])
  plt.ylim([-0.05, 1.05])
  plt.ylabel('Taxa de Verdadeiro Positivo',size=15)
  plt.xlabel('Taxa de Falso Positivo',size=15)
  plt.xticks(fontsize= 12)
  plt.yticks(fontsize= 12)
  plt.savefig("comparacao_de_curvas_roc.png", bbox_inches = "tight")

clf_names = [
   "kNN1",
   "kNN2",
   "kNN3",
   "kNN4",
   "NB1",
   "NB2",
   "AD1",
   "AD2",
   "AD3",
   "AD4",
   "Stacking"
]

dataset_names = [
    'normalized', 
    'pca', 
    'pca',
    'pca', 
    'default', 
    'pca',
    'default', 
    'normalized', 
    'normalized',
    'normalized',
    'default'
]

clf_list = [
    KNeighborsClassifier(n_neighbors=11),
    KNeighborsClassifier(n_neighbors=1),
    KNeighborsClassifier(n_neighbors=9),
    KNeighborsClassifier(n_neighbors=20),
    GaussianNB(),
    GaussianNB(),
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=7),
    DecisionTreeClassifier(max_depth=2),
    DecisionTreeClassifier(max_depth=20),
    sclf
]

param_descs = [
   "k=%0.0f" % (clf_list[0].n_neighbors),
   "k=%0.0f" % (clf_list[1].n_neighbors),
   "k=%0.0f" % (clf_list[2].n_neighbors),
   "k=%0.0f" % (clf_list[3].n_neighbors),
   "var_smoothing=%0.10f" % (clf_list[4].var_smoothing),
   "var_smoothing=%0.10f" % (clf_list[5].var_smoothing),
   "max_depth=%0.0f" % (clf_list[6].max_depth),
   "max_depth=%0.0f" % (clf_list[7].max_depth),
   "max_depth=%0.0f" % (clf_list[8].max_depth),
   "max_depth=%0.0f" % (clf_list[9].max_depth),
   ""
]



X_list = [
   X_norm,
   X_pca,
   X_pca,
   X_pca,
   X,
   X_pca,
   X,
   X_norm,
   X_norm,
   X_norm,
   X
]

gerar_comparacao_curva_roc(clf_names, param_descs, dataset_names, clf_list, X_list, y, stratifiedKfold)